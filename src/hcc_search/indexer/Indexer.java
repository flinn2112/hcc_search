/*
 * To change this template, choose Tools | Templates
 * and open the template in the editor.
 */
package hcc_search.indexer;
import hcc_search.hResult;
import hcc_search.hcc_exception;
import hcc_search.hcc_utils;
import hcc_search.myClassLoader;
import org.apache.lucene.analysis.Analyzer;
//import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.standard.* ;


import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
//import org.apache.lucene.document.NumericField;
//import org.apache.lucene.index.FieldInfo.IndexOptions;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig.OpenMode;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import java.nio.file.Paths;

import java.io.File;

import java.io.IOException;
import java.io.InputStream;
import java.util.Date ;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.SAXException;

/**
 *
 * @author frankkempf
 * Indexes most things...
 */
public class Indexer {
    IndexWriter m_idxWriter = null ;
    private String  m_strIndexPath;
    private Integer m_iTraceLevel = 0 ;
    private boolean m_bCreate ;
    
    public static final int STATUS_UNKNOWN                  =      0 ;
    public static final int STATUS_OK                       =   1001 ;
    public static final int STATUS_ERR                      =   1002 ;
    public static final int ERR_IDX_ALL_GOOD                =      1 ;
    public static final int ERR_IDX_NOT_HANDLED             =  -1001 ;
    public static final int ERR_IDX_FILE_NOT_ACCESSIBLE     = -11001 ;
    public static final int ERR_IDX_FILE_NO_EXT             = -11002 ;
    public static final int ERR_IDX_FILE_EXTENSION_EXCLUDED = -11003 ;
    
    public Indexer( String strIndexPath, boolean bCreate, Integer iTraceLevel){
        m_strIndexPath = strIndexPath ;
        m_bCreate      = bCreate;
        m_iTraceLevel  = iTraceLevel ;
        m_idxWriter = this.openIndex() ;              
    }
    public IndexWriter openIndex(){        
        Date start = new Date();
        try {
            System.out.println("Index directory '" + m_strIndexPath + "'...");
            Directory dir = FSDirectory.open( Paths.get(m_strIndexPath) );
            Analyzer analyzer = new StandardAnalyzer( );
            IndexWriterConfig iwc = new IndexWriterConfig(analyzer);

            if (m_bCreate) {
              // Create a new index in the directory, removing any
              // previously indexed documents:
              iwc.setOpenMode(OpenMode.CREATE);
            } else {
              // Add new documents to an existing index:
              iwc.setOpenMode(OpenMode.CREATE_OR_APPEND);
            }

      // Optional: for better indexing performance, if you
      // are indexing many documents, increase the RAM
      // buffer.  But if you do this, increase the max heap
      // size to the JVM (eg add -Xmx512m or -Xmx1g):
      //
      // iwc.setRAMBufferSizeMB(256.0);
      iwc.setMaxBufferedDocs(8192) ;
      iwc.setCommitOnClose(true) ;
      m_idxWriter = new IndexWriter(dir, iwc);
      //m_idxWriter.setRAMBufferSizeMB(384);
      //m_idxWriter.setMaxBufferedDocs(50);
      } catch (IOException e) {          
        System.out.println("Cannot create idxWriter '" + e.toString() + "'...");
        m_idxWriter = null ; 
      }
      return  m_idxWriter ;
    }
    
    public boolean closeIndex(){
      System.out.println("!---------------------->Indexer::closeIndex: closing....<---------------------------- ");      
      if( null == this.m_idxWriter ) {
        System.out.println("Error! idxWriter is null. Cannot complete operation ");     
        return false ;
      }
      
      
      
      try{  
          //commit wurde Ã¼ber setCommitOnClose eingeschaltet.
          //System.out.println("Indexer::closeIndex: calling commit.... ");   
          //this.m_idxWriter.commit();
          System.out.println("Indexer::closeIndex: changes committed. ");
          this.m_idxWriter.close();  
          Date end = new Date();
          //System.out.println(end.getTime() - start.getTime() + " total milliseconds");
        }
      catch(org.apache.lucene.store.AlreadyClosedException eAC){
          System.out.println("---------->  !!!!!!!!!!!!!!!!! caught a " + eAC.getClass() +
           "\n with message: " + eAC.getMessage());
      }  catch (IOException e) {
          System.out.println("---------->  !!!!!!!!!!!!!!!!!  caught a " + e.getClass() +
           "\n with message: " + e.getMessage());
        }   
        return true ;
    }
    
    
      /**
   * Indexes the given file using the given writer, or if a directory is given,
   * recurses over files and directories found under the given directory.
   * 
   * NOTE: This method indexes one document per input file.  This is slow.  For good
   * throughput, put multiple documents into your input file(s).  An example of this is
   * in the benchmark module, which can create "line doc" files, one document per line,
   * using the
   * <a href="../../../../../contrib-benchmark/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.html"
   * >WriteLineDocTask</a>.
   *  
   * @param writer Writer to the index where the given file/dir info will be stored
   * @param file The file to index, or the directory to recurse into to find files to index
   * @throws IOException
   */
  //This method was superseeded by extractTextTika in 1.8.3.1
     public hResult extractText(File file,  Long lTotalFiles, Long lCurrentFile) throws IOException {
        hResult   oResult = new hResult() ; //search Result object
        
        String strTmp = null;
        String strClassName = null ;
        Integer iID ;        
        myClassLoader ccl = new myClassLoader(0) ; //0=no trace
        
        
        oResult.m_iStatusCode = hResult.STATUS_UNKNOWN  ;
        
         if (false == file.canRead()) {             
              return oResult ;
         }

         //it is a file - get extension and call extractor
            
            
            /*
             if( null == oResult.m_strExt || 0 == oResult.m_strExt.length()  ){
                 oResult.m_iStatusCode =  hResult.ERR_IDX_FILE_NO_EXT ;
                 return oResult ;
             }
             */

             if( false == hcc_utils.isContained(hcc_utils.getExtensionUpper(file))){
                 System.out.println("no file extension configured [" + file.getName() + "]" ) ;
                 oResult.m_iStatusCode =  hResult.ERR_IDX_FILE_EXTENSION_EXCLUDED ;
                 return oResult ;
             }
             System.out.println("Processing [" + file.getName() + "] [" 
                     + lTotalFiles.toString() + "/" + lCurrentFile.toString() + "]" ) ;
             //compile classname
             strClassName = "hcc_search." + hcc_utils.getExtensionUpper(file) + "Extractor" ;
             //do  we have such a class?
             if( null == ccl.findClass(strClassName) ){
              strClassName = "hcc_search.BINExtractor" ;
             }

             try{
                     oResult = (hResult)ccl.invokeClassMethod(strClassName, "processDocument", file) ;
                     switch (oResult.m_eRetType){
                         case hResult.DOCUMENT :
                             oResult.m_iStatusCode = hResult.STATUS_OK ;
                             break ;
                         case hResult.CONTENT: 
                         case hResult.UNKNOWN : //a string
                             oResult.m_iStatusCode = hResult.STATUS_OK ;
                              break ; 
                         default:                         
                              oResult.m_iStatusCode = hResult.STATUS_ERR ; //nothing to do
                     }

                }
                catch(Exception ex){
                    Throwable t = ex.getCause() ;
                    System.out.println("indexFile failed [" + ex.toString() + "]" ) ;
                    throw new IOException( ex.toString() ) ;
                }
             
        if(file.isDirectory()){
                oResult.m_strType = "DIR" ;
        }     
        oResult.m_strFilename = file.getName() ;
        oResult.m_strPath = file.getPath() ;
        oResult.m_lLastModified = file.lastModified() ;
        oResult.m_strLastModified = hcc_utils.getLastModifiedString(file) ;
        oResult.m_strYear = hcc_utils.getYear(file) ;
        oResult.m_strExt = hcc_utils.getExtensionUpper(file) ;
        oResult.m_lFileLength = file.length() ;
        
        return oResult ;
     }
 
     /*
        1.8.3.1 check extracted Textlen is significantly lower than filesize,
        then it could be suspect to review.
        For HTML files there is always an overhead of tags, for PDF less.
        In the first version we concentrate on PDFs.        
     */
     public static boolean poorContent(hResult oResult, int iThresholdSize, int iFactor){
         boolean bRet = false ;
         
         //System.out.println(oResult.m_strExt.toLowerCase());
         
         if( oResult.m_strExt.toLowerCase().matches( "pdf") ){
           if( (oResult.m_iPayLoadLen < iThresholdSize) || (oResult.m_iPayLoadLen * iFactor) < oResult.m_lFileLength ){
               bRet = true ;
           }
         }
         return bRet ;
     }
     
     
      /*
        2018 Apache Tika Extractor
   __  _______ _____   ________   ____________ __ ___ 
  / / / / ___//  _/ | / / ____/  /_  __/  _/ //_//   |
 / / / /\__ \ / //  |/ / / __     / /  / // ,<  / /| |
/ /_/ /___/ // // /|  / /_/ /    / / _/ // /| |/ ___ |
\____//____/___/_/ |_/\____/    /_/ /___/_/ |_/_/  |_|
 */
     
     private static String parseUsingAutoDetect(String filename, int iLimit) throws Exception {
        System.out.println("Handling using AutoDetectParser: [" + filename + "]");
        TikaConfig         tikaConfig    = TikaConfig.getDefaultConfig();
        AutoDetectParser parser = new AutoDetectParser(tikaConfig);
        BodyContentHandler handler = new BodyContentHandler(iLimit);
        Metadata           metadata      = new Metadata();
        TikaInputStream stream = TikaInputStream.get(new File(filename), metadata);
        parser.parse(stream, handler, metadata, new ParseContext());
        return handler.toString();
    }
     
     
     public hResult extractTextTika(File file, int iLimitLen, Long lTotalFiles, Long lCurrentFile) throws IOException, SAXException, TikaException {
            hResult   oResult = new hResult() ; //search Result object      
            
             //org.apache.tika
            oResult.m_iStatusCode = hResult.STATUS_ERR ; //DEFAULT
           
            try{
               oResult.m_oPayLoad = Indexer.parseUsingAutoDetect(file.getAbsolutePath(), iLimitLen) ;
                       //com.hcc_medical.hcctika.extract.extractTextTika(file.getAbsolutePath(), lTotalFiles, lCurrentFile) ;
               oResult.m_iStatusCode = hResult.STATUS_OK ;
               oResult.m_eRetType = hResult.CONTENT ;
               oResult.m_iPayLoadLen = ((String)oResult.m_oPayLoad).length() ;
            }catch(Exception ex){
               System.err.println(ex.toString()) ;
            }finally{
                
            }
            
            
     System.out.println("Tika Extracted [" + oResult.m_oPayLoad.toString().length() + "] bytes.");
     
    /* 
        if( oResult.m_oPayLoad.toString().length() < 1000 ){
             System.out.println( oResult.m_oPayLoad.toString()) ;
        }  
      */  
            if(file.isDirectory()){
                oResult.m_strType = "DIR" ;
            }     
            oResult.m_strFilename = file.getName() ;
            oResult.m_strPath = file.getPath() ;
            oResult.m_lLastModified = file.lastModified() ;
            oResult.m_strLastModified = hcc_utils.getLastModifiedString(file) ;
            oResult.m_strYear = hcc_utils.getYear(file) ;
            oResult.m_strExt = hcc_utils.getExtensionUpper(file) ;
            oResult.m_lFileLength = file.length() ;
            
            return  oResult ;
     }
  
  
      public int obsolete_indexFile(IndexWriter writer, hResult oResult) 
        throws IOException {
           
            String strTmp = null;
            Integer iID ;
            String strDate = new String("") ;
            Field fDocField = null ; //Lucene attribute field
            Document doc = null ;
       
             
            try {   

              // Add the path of the file as a field named "path".  Use a
              // field that is indexed (i.e. searchable), but don't tokenize 
              // the field into separate words and don't index term frequency
              // or positional information:
              Field pathField = new Field("path", oResult.m_strPath, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
              //pathField.setIndexOptions(IndexOptions.DOCS_ONLY);
              doc.add(pathField);
              strDate = oResult.m_strLastModified ; 
              Field dateField = new Field("date",  strDate, 
                                    Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
              //dateField.setIndexOptions(IndexOptions.DOCS_ONLY);
              doc.add(dateField);
              //System.out.println("adding field lastModified, value [" + strDate + "]");
              strTmp = System.getenv("COMPUTERNAME") ;
              if( null != strTmp && strTmp.length() == 0){
                   strTmp = System.getenv("HOSTNAME") ;
              }
              if( null != strTmp && strTmp.length() > 0 ){
                  Field hostName = new Field("source", oResult.m_strPath, Field.Store.YES, Field.Index.ANALYZED);
                  //pathField.setIndexOptions(IndexOptions.DOCS_ONLY);
                  doc.add(hostName);                     
              }

              fDocField = new Field("doctype", oResult.m_strExt, Field.Store.YES, Field.Index.ANALYZED);
              //fDocField.setIndexOptions(IndexOptions.DOCS_ONLY);
              doc.add(fDocField);  

              //the Filename (without path) will be included as field, 
              //so users may search for parts of it (i.e. name:CV*).
              //Useful when searching for documents and remember parts of its name.
              fDocField = new Field("filename", hcc_utils.getFilename(oResult.m_strPath), Field.Store.YES, Field.Index.ANALYZED);
              //fDocField.setIndexOptions(IndexOptions.DOCS_ONLY);
              doc.add(fDocField);  

              fDocField = new Field("year", oResult.m_strYear, Field.Store.YES, Field.Index.ANALYZED);
              //fDocField.setIndexOptions(IndexOptions.DOCS_ONLY);
              doc.add(fDocField);  

              // Add the last modified date of the file a field named "modified".
              // Use a NumericField that is indexed (i.e. efficiently filterable with
              // NumericRangeFilter).  This indexes to milli-second resolution, which
              // is often too fine.  You could instead create a number based on
              // year/month/day/hour/minutes/seconds, down the resolution you require.
              // For example the long value 2011021714 would mean
              // February 17, 2011, 2-3 PM.
              
            
            
              org.apache.lucene.document.LongField modifiedField = 
                  new org.apache.lucene.document.LongField("lastModified", oResult.m_lLastModified , 
                                           org.apache.lucene.document.Field.Store.YES) ;
              doc.add(modifiedField);
      /*        
              NumericField sizeField = new NumericField("size");
              modifiedField.setLongValue(file.length());
              doc.add(sizeField);
    */
              // Add the contents of the file to a field named "contents".  Specify a Reader,
              // so that the text of the file is tokenized and indexed, but not stored.
              // Note that FileReader expects the file to be in UTF-8 encoding.
              // If that's not the case searching for special characters will fail.


              if (writer.getConfig().getOpenMode() == OpenMode.CREATE) {
                // New index, so we just add the document (no old document can be there):
                  if( m_iTraceLevel > 2)
                    System.out.println("adding file [" + oResult.m_strFilename + "]");
                    try{
                        writer.addDocument(doc);
                    }catch (java.lang.OutOfMemoryError ex){
                        System.out.println("Ex " + ex.toString());
                         throw new IOException( ex.toString() ) ;
                    }
                }
               else {
                // Existing index (an old copy of this document may have been indexed) so 
                // we use updateDocument instead to replace the old one matching the exact 
                // path, if present:
                System.out.println("updating " + oResult.m_strFilename);
                writer.updateDocument(new Term("path", oResult.m_strPath), doc);
              }
            } finally {

            }     
            return ERR_IDX_ALL_GOOD ;
      }//indexFile
      
      public void indexData( IndexData oIdxData ) throws hcc_exception
         {
            Field fDocField = null ; //Lucene attribute field
            Document doc = null ;
            IndexField idxField = null ;
            Term oTerm = null ;

             try{                   

                  doc = new Document();
                  doc.add(new Field("contents", oIdxData.m_strText,
                            Field.Store.YES, Field.Index.ANALYZED));
                }
                catch(Exception ex){
                    Throwable t = ex.getCause() ;
                    System.out.println("Indexer::indexData failed [" + ex.toString() + "]" ) ;
                    return ;
                }


//              These fields are not tokenized.
                try {   
                   for(int i=0;i<oIdxData.m_VFields.size();i++)
                   {
                      idxField = oIdxData.getField(i) ;                      
                      if( null == idxField){
                          continue ;
                      }
                      fDocField = new Field( idxField.m_strName, idxField.m_strValue, 
                              Field.Store.YES, Field.Index.NOT_ANALYZED);
                     // fDocField.setIndexOptions(IndexOptions.DOCS_ONLY);
                      doc.add(fDocField);  
                   }
              
              // Add the contents of the file to a field named "contents".  Specify a Reader,
              // so that the text of the file is tokenized and indexed, but not stored.
              // Note that FileReader expects the file to be in UTF-8 encoding.
              // If that's not the case searching for special characters will fail.


              if (this.m_idxWriter.getConfig().getOpenMode() == OpenMode.CREATE) {
                // New index, so we just add the document (no old document can be there):
                  if( m_iTraceLevel > 2)
                    System.out.println("adding data [" + oIdxData.m_strURN + "]");
                    try{
                        this.m_idxWriter.addDocument(doc);
                        //this.m_idxWriter.updateDocument(null, doc);
                    }catch (java.lang.OutOfMemoryError ex){

                        System.out.println("Ex " + ex.toString());
                    }
                   catch (Exception ex2){

                        System.out.println("Ex " + ex2.toString());
                    } 
                }
               else { //update
                // Existing index (an old copy of this document may have been indexed) so 
                // we use updateDocument instead to replace the old one matching the exact 
                // path, if present:
                  if( m_iTraceLevel > 2)
                     System.out.println("updating " + oIdxData.m_strURN );
                try{                    
                    oTerm = new Term("ID", oIdxData.m_strID) ;
                    //if( m_iTraceLevel > 2)
                        System.out.println("Term for update is:" + oTerm.text()) ;
                    this.m_idxWriter.updateDocument(oTerm, doc);
                }catch(Exception ex){
                    System.out.println(ex.toString()) ;
                }
              }
            } finally {

            }     
      }//indexData
      
    }

 
    
